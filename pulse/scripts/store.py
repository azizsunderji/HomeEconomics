"""SQLite storage for Pulse items, classifications, and arc tracking."""

from __future__ import annotations

import json
import sqlite3
from datetime import datetime, timezone, timedelta
from pathlib import Path
from typing import Optional

from collectors import PulseItem

DEFAULT_DB_PATH = Path(__file__).parent.parent / "data" / "pulse.db"


def get_db(db_path: Optional[Path] = None) -> sqlite3.Connection:
    """Open (and optionally create) the Pulse database."""
    path = db_path or DEFAULT_DB_PATH
    path.parent.mkdir(parents=True, exist_ok=True)
    conn = sqlite3.connect(str(path))
    conn.row_factory = sqlite3.Row
    conn.execute("PRAGMA journal_mode=WAL")
    conn.execute("PRAGMA foreign_keys=ON")
    _ensure_schema(conn)
    return conn


def _ensure_schema(conn: sqlite3.Connection) -> None:
    """Create tables if they don't exist."""
    conn.executescript("""
        CREATE TABLE IF NOT EXISTS items (
            id INTEGER PRIMARY KEY AUTOINCREMENT,
            source TEXT NOT NULL,
            source_id TEXT NOT NULL,
            content_hash TEXT NOT NULL,
            url TEXT NOT NULL,
            title TEXT NOT NULL,
            body TEXT DEFAULT '',
            author TEXT DEFAULT '',
            published_at TEXT,
            collected_at TEXT NOT NULL,
            score INTEGER DEFAULT 0,
            num_comments INTEGER DEFAULT 0,
            engagement_raw TEXT DEFAULT '{}',
            subreddit TEXT DEFAULT '',
            platform_tags TEXT DEFAULT '[]',
            feed_name TEXT DEFAULT '',
            feed_priority TEXT DEFAULT '',

            -- Classification fields (filled by Haiku)
            topics TEXT DEFAULT '[]',
            relevance_score INTEGER,
            entities TEXT DEFAULT '[]',
            extracted_stats TEXT DEFAULT '[]',
            sentiment TEXT DEFAULT '',
            classified_at TEXT,

            -- Dedup
            UNIQUE(source, source_id)
        );

        CREATE INDEX IF NOT EXISTS idx_items_collected
            ON items(collected_at);
        CREATE INDEX IF NOT EXISTS idx_items_content_hash
            ON items(content_hash);
        CREATE INDEX IF NOT EXISTS idx_items_relevance
            ON items(relevance_score);
        CREATE INDEX IF NOT EXISTS idx_items_source
            ON items(source);

        -- Topic arc tracking (rolling windows)
        CREATE TABLE IF NOT EXISTS topic_arcs (
            id INTEGER PRIMARY KEY AUTOINCREMENT,
            topic TEXT NOT NULL,
            date TEXT NOT NULL,  -- YYYY-MM-DD
            item_count INTEGER DEFAULT 0,
            avg_relevance REAL DEFAULT 0,
            avg_sentiment_score REAL DEFAULT 0,  -- -1 to 1
            platform_count INTEGER DEFAULT 0,    -- how many platforms mentioned it
            platforms TEXT DEFAULT '[]',
            top_item_ids TEXT DEFAULT '[]',       -- IDs of highest-relevance items

            UNIQUE(topic, date)
        );

        -- Story opportunities (generated by Sonnet)
        CREATE TABLE IF NOT EXISTS story_opportunities (
            id INTEGER PRIMARY KEY AUTOINCREMENT,
            created_at TEXT NOT NULL,
            headline TEXT NOT NULL,
            topic TEXT NOT NULL,
            summary TEXT NOT NULL,
            data_sources TEXT DEFAULT '[]',       -- data lake files relevant
            urgency TEXT DEFAULT 'normal',        -- low, normal, high, breaking
            time_estimate TEXT DEFAULT '',
            contrarian_angle TEXT DEFAULT '',
            source_item_ids TEXT DEFAULT '[]',    -- items that inspired this
            pushed_to_notion INTEGER DEFAULT 0,
            notion_page_id TEXT DEFAULT ''
        );

        -- Briefing archive
        CREATE TABLE IF NOT EXISTS briefings (
            id INTEGER PRIMARY KEY AUTOINCREMENT,
            created_at TEXT NOT NULL,
            briefing_type TEXT NOT NULL,  -- daily, weekly
            content_json TEXT NOT NULL,   -- full structured briefing
            email_sent INTEGER DEFAULT 0,
            email_sent_at TEXT
        );

        -- Collection run log
        CREATE TABLE IF NOT EXISTS collection_runs (
            id INTEGER PRIMARY KEY AUTOINCREMENT,
            started_at TEXT NOT NULL,
            completed_at TEXT,
            source TEXT NOT NULL,
            items_collected INTEGER DEFAULT 0,
            items_new INTEGER DEFAULT 0,
            items_duplicate INTEGER DEFAULT 0,
            error TEXT DEFAULT ''
        );
    """)
    conn.commit()

    # Schema migration: add conversation-pivot columns (safe to re-run)
    _safe_add_columns(conn)


def _safe_add_columns(conn: sqlite3.Connection) -> None:
    """Add new columns if they don't exist (conversation pivot migration)."""
    existing = {row[1] for row in conn.execute("PRAGMA table_info(items)").fetchall()}
    migrations = [
        ("content_type", "TEXT DEFAULT ''"),
        ("conversation_signal", "INTEGER DEFAULT 0"),
        ("verifiable_claims", "TEXT DEFAULT '[]'"),
    ]
    for col_name, col_def in migrations:
        if col_name not in existing:
            conn.execute(f"ALTER TABLE items ADD COLUMN {col_name} {col_def}")
    conn.commit()


# ── Item CRUD ─────────────────────────────────────────────────────────────────

def upsert_item(conn: sqlite3.Connection, item: PulseItem) -> tuple[int, bool]:
    """Insert item if new, skip if duplicate. Returns (row_id, is_new)."""
    d = item.to_dict()
    d["content_hash"] = item.content_hash

    # Check for exact source+source_id dupe
    existing = conn.execute(
        "SELECT id FROM items WHERE source = ? AND source_id = ?",
        (d["source"], d["source_id"])
    ).fetchone()

    if existing:
        return existing["id"], False

    # Check for content hash dupe (cross-platform)
    hash_dupe = conn.execute(
        "SELECT id FROM items WHERE content_hash = ?",
        (d["content_hash"],)
    ).fetchone()
    # Still insert — we want to track cross-platform appearances — but note it
    is_content_dupe = hash_dupe is not None

    cols = list(d.keys())
    placeholders = ", ".join(["?"] * len(cols))
    col_names = ", ".join(cols)

    cursor = conn.execute(
        f"INSERT OR IGNORE INTO items ({col_names}) VALUES ({placeholders})",
        [d[c] for c in cols]
    )
    conn.commit()

    if cursor.lastrowid:
        return cursor.lastrowid, True
    # If INSERT OR IGNORE skipped, find the existing row
    row = conn.execute(
        "SELECT id FROM items WHERE source = ? AND source_id = ?",
        (d["source"], d["source_id"])
    ).fetchone()
    return row["id"] if row else 0, False


def bulk_upsert(conn: sqlite3.Connection, items: list[PulseItem]) -> tuple[int, int]:
    """Upsert many items. Returns (new_count, dupe_count)."""
    new_count = 0
    dupe_count = 0
    for item in items:
        _, is_new = upsert_item(conn, item)
        if is_new:
            new_count += 1
        else:
            dupe_count += 1
    return new_count, dupe_count


def get_unclassified(conn: sqlite3.Connection, limit: int = 500) -> list[dict]:
    """Get items that haven't been classified yet."""
    rows = conn.execute(
        "SELECT * FROM items WHERE classified_at IS NULL ORDER BY collected_at DESC LIMIT ?",
        (limit,)
    ).fetchall()
    return [dict(r) for r in rows]


def update_classification(
    conn: sqlite3.Connection,
    item_id: int,
    topics: list[str],
    relevance_score: int,
    entities: list[str],
    extracted_stats: list[str],
    sentiment: str,
) -> None:
    """Update an item with Haiku classification results."""
    now = datetime.now(timezone.utc).isoformat()
    conn.execute(
        """UPDATE items SET
            topics = ?, relevance_score = ?, entities = ?,
            extracted_stats = ?, sentiment = ?, classified_at = ?
        WHERE id = ?""",
        (
            json.dumps(topics), relevance_score, json.dumps(entities),
            json.dumps(extracted_stats), sentiment, now, item_id
        )
    )
    conn.commit()


def update_conversation_classification(
    conn: sqlite3.Connection,
    item_id: int,
    content_type: str,
    conversation_signal: int,
    verifiable_claims: list[str],
) -> None:
    """Update an item with conversation-pivot classification fields."""
    conn.execute(
        """UPDATE items SET
            content_type = ?, conversation_signal = ?, verifiable_claims = ?
        WHERE id = ?""",
        (content_type, conversation_signal, json.dumps(verifiable_claims), item_id)
    )
    conn.commit()


def get_items_since(
    conn: sqlite3.Connection,
    hours: int = 24,
    min_relevance: Optional[int] = None,
) -> list[dict]:
    """Get classified items published in the last N hours.

    Uses published_at when available, falls back to collected_at.
    This ensures stale articles collected late don't dominate the briefing.
    """
    cutoff = (datetime.now(timezone.utc) - timedelta(hours=hours)).isoformat()
    query = """SELECT * FROM items
        WHERE classified_at IS NOT NULL
        AND COALESCE(NULLIF(published_at, ''), collected_at) >= ?"""
    params: list = [cutoff]

    if min_relevance is not None:
        query += " AND relevance_score >= ?"
        params.append(min_relevance)

    query += " ORDER BY relevance_score DESC"
    rows = conn.execute(query, params).fetchall()
    return [dict(r) for r in rows]


def get_conversation_items(
    conn: sqlite3.Connection,
    hours: int = 36,
    min_conversation_signal: int = 30,
) -> list[dict]:
    """Get items with high conversation signal (organic discussion)."""
    cutoff = (datetime.now(timezone.utc) - timedelta(hours=hours)).isoformat()
    rows = conn.execute(
        """SELECT * FROM items
        WHERE classified_at IS NOT NULL
        AND conversation_signal >= ?
        AND COALESCE(NULLIF(published_at, ''), collected_at) >= ?
        ORDER BY conversation_signal DESC, num_comments DESC""",
        (min_conversation_signal, cutoff)
    ).fetchall()
    return [dict(r) for r in rows]


# ── Topic arcs ────────────────────────────────────────────────────────────────

def update_topic_arc(
    conn: sqlite3.Connection,
    topic: str,
    date: str,
    item_count: int,
    avg_relevance: float,
    avg_sentiment: float,
    platforms: list[str],
    top_item_ids: list[int],
) -> None:
    """Upsert daily topic arc entry."""
    conn.execute(
        """INSERT INTO topic_arcs (topic, date, item_count, avg_relevance,
            avg_sentiment_score, platform_count, platforms, top_item_ids)
        VALUES (?, ?, ?, ?, ?, ?, ?, ?)
        ON CONFLICT(topic, date) DO UPDATE SET
            item_count = excluded.item_count,
            avg_relevance = excluded.avg_relevance,
            avg_sentiment_score = excluded.avg_sentiment_score,
            platform_count = excluded.platform_count,
            platforms = excluded.platforms,
            top_item_ids = excluded.top_item_ids""",
        (
            topic, date, item_count, avg_relevance, avg_sentiment,
            len(platforms), json.dumps(platforms), json.dumps(top_item_ids)
        )
    )
    conn.commit()


def get_topic_arc(
    conn: sqlite3.Connection,
    topic: str,
    days: int = 30,
) -> list[dict]:
    """Get topic arc data for the last N days."""
    cutoff = (datetime.now(timezone.utc) - timedelta(days=days)).strftime("%Y-%m-%d")
    rows = conn.execute(
        "SELECT * FROM topic_arcs WHERE topic = ? AND date >= ? ORDER BY date",
        (topic, cutoff)
    ).fetchall()
    return [dict(r) for r in rows]


# ── Story opportunities ───────────────────────────────────────────────────────

def add_story_opportunity(conn: sqlite3.Connection, **kwargs) -> int:
    """Insert a story opportunity. Returns row ID."""
    kwargs.setdefault("created_at", datetime.now(timezone.utc).isoformat())
    for list_field in ("data_sources", "source_item_ids"):
        if list_field in kwargs and isinstance(kwargs[list_field], list):
            kwargs[list_field] = json.dumps(kwargs[list_field])
    cols = list(kwargs.keys())
    placeholders = ", ".join(["?"] * len(cols))
    col_names = ", ".join(cols)
    cursor = conn.execute(
        f"INSERT INTO story_opportunities ({col_names}) VALUES ({placeholders})",
        [kwargs[c] for c in cols]
    )
    conn.commit()
    return cursor.lastrowid


def get_unpushed_stories(conn: sqlite3.Connection) -> list[dict]:
    """Get story opportunities not yet pushed to Notion."""
    rows = conn.execute(
        "SELECT * FROM story_opportunities WHERE pushed_to_notion = 0 ORDER BY created_at DESC"
    ).fetchall()
    return [dict(r) for r in rows]


# ── Collection runs ───────────────────────────────────────────────────────────

def log_collection_start(conn: sqlite3.Connection, source: str) -> int:
    """Log the start of a collection run. Returns run ID."""
    now = datetime.now(timezone.utc).isoformat()
    cursor = conn.execute(
        "INSERT INTO collection_runs (started_at, source) VALUES (?, ?)",
        (now, source)
    )
    conn.commit()
    return cursor.lastrowid


def log_collection_end(
    conn: sqlite3.Connection,
    run_id: int,
    items_collected: int,
    items_new: int,
    items_duplicate: int,
    error: str = "",
) -> None:
    """Log the end of a collection run."""
    now = datetime.now(timezone.utc).isoformat()
    conn.execute(
        """UPDATE collection_runs SET
            completed_at = ?, items_collected = ?, items_new = ?,
            items_duplicate = ?, error = ?
        WHERE id = ?""",
        (now, items_collected, items_new, items_duplicate, error, run_id)
    )
    conn.commit()


# ── Briefings ─────────────────────────────────────────────────────────────────

def save_briefing(conn: sqlite3.Connection, briefing_type: str, content: dict) -> int:
    """Save a generated briefing."""
    now = datetime.now(timezone.utc).isoformat()
    cursor = conn.execute(
        "INSERT INTO briefings (created_at, briefing_type, content_json) VALUES (?, ?, ?)",
        (now, briefing_type, json.dumps(content))
    )
    conn.commit()
    return cursor.lastrowid


def mark_briefing_emailed(conn: sqlite3.Connection, briefing_id: int) -> None:
    """Mark a briefing as emailed."""
    now = datetime.now(timezone.utc).isoformat()
    conn.execute(
        "UPDATE briefings SET email_sent = 1, email_sent_at = ? WHERE id = ?",
        (now, briefing_id)
    )
    conn.commit()


# ── Stats ─────────────────────────────────────────────────────────────────────

def get_collection_stats(conn: sqlite3.Connection, hours: int = 24) -> dict:
    """Get collection stats for the last N hours."""
    cutoff = (datetime.now(timezone.utc) - timedelta(hours=hours)).isoformat()
    rows = conn.execute(
        """SELECT source,
            SUM(items_new) as total_new,
            SUM(items_duplicate) as total_dupe,
            SUM(items_collected) as total_collected,
            COUNT(*) as runs
        FROM collection_runs
        WHERE started_at >= ?
        GROUP BY source""",
        (cutoff,)
    ).fetchall()
    return {r["source"]: dict(r) for r in rows}
